# 深入理解kafka

## 	1. kafka如何复制

​	broker 有一个唯一标识符(id)可以指定，也可以自动生成。通过注册到zookeeper下临时节点。来通知集群，有新加或者节点退出。

​	如果发生了长时间的垃圾回收或者网络分区之类。broker会从zookeeper的临时节点中删除。这个时候如果启动一个和被删除的节点一样的ID的broker，它会立即加入集群并含有和旧的broker的一样的主题和分区。

​	<font color="red">控制器就是一个特殊的broker</font>，除了正常的broker，还有分区首领的选举功能。通过在/Controller 下面创建临时节点来成为控制器。当有broker挂掉的时候，控制器会知道，然后如果有分区首领在这个broker上，就获取分区副本的下一个分区副本成为新的分区首领。

​	控制器通过epoch来避免脑裂。

​	副本有两种类型：

		1. 首领副本
  		2. 跟随者副本



​	跟随者只会从首领副本获取消息，如果跟随者副本在10s内，没有请求最新消息或者没有请求消息。他就不可能在首领副本失效的时候成为新的首领，反之持续同步的就是同步副本。**<font color="red">只有同步副本才可能成为新的首领（也就是所谓的ISR）。</font>**

​	首选首领：创建主题的时候指定的首领就是首选首领。希望在首选首领成为分区首领的情况下，负载是基本均衡的。

## 	2. kafaka如何处理消费者和生产者的请求	

​	因为所有的生产请求和消费请求都必须发送到首领副本，客户端就必须知道每一个分区的首领副本对应的broker。通过发送元数据请求来获取这些元数据信息。

​	kafka处理客户端请求的全过程图：

​	![kafka处理请求内部流程图](../images/kafka处理请求内部流程图.png)

​	客户端需要自己负责把自己的请求发送到正确的首领副本所在broker上去。这样就需要元数据信息的请求。也就是客户端，需要订阅的所有的主题的分区数目和首领分区的所在broker的列表。**<font color="red">所有的broker都有这些元数据信息</font>**。那么元数据信息的请求可以发送到任意一个broker上。

![客户端路由请求](../images/客户端路由请求.png)

​	生产请求：副本首领的broker收到消息请求以后，会首先做些校验，比如写入权限和ack参数有效性，Linux系统会**首先写入到系统缓存中，并不保证何时刷新到磁盘上的**。如果acks参数是all请求会被放入到炼狱缓冲区中。直到所有的副本都收到消息，才返回给客户端。

​	获取请求：客户端向broker请求主题分区中的特定偏移量的消息。客户端可以指定在broker从一个分区中最多返回多少数据。因为客户端需要给broker返回的数据分配足够的内存。**kafka使用零拷贝技术来给客户端发送消息，就是从Linux文件系统缓存直接到网络通道**。而且大部分客户端只可以获取到已经写入到同步副本的消息。

![image-20190804180441430](../images/image-20190804180441430.png)

​	对于获取数据的请求，获取数据可以设置获取数据的下限和上限。客户端可以设置超时时间，超过这个时间即便没有达到数据量的下限也会返回数据。

​	其他请求：

## 	kafka存储细节

​	kafka 的基本存储单位是分区。

# kafka 的分区分配



# kafka的文件管理

kafka不会一直保留文件。管理员可以规定数据被删除之前可以保留多长时间，或者是最多保留多大的数据。

​	为了提升查找的效率，kafka把文件分成多个段。默认是一个G或者是一周的数据。当前写入的片段成为活跃片段。是不可以被删除。

# kafka文件格式



文件格式如果是压缩过的，会有更好的传输存储性能。

# kafka 索引

kafka给每一个分区维护了索引。索引也被分段了。删除消息的时候也会删除对应的索引。

# kafka清理

